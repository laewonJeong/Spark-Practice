{
  "metadata": {
    "name": "SparkSQL-Practice",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfrom pyspark.sql import SparkSession\n\nspark \u003d SparkSession.builder.appName(\"SparkSQL-Pr.\").getOrCreate()\ndf \u003d spark.read.option(\"header\", \"true\").csv(f\"hdfs://MN:9000/data/champion_81.csv\")\ndf \u003d df.dropDuplicates()\ndf.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.select(\"win\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import col\n\ndf.filter(col(\"total_damage_to_champion\") \u003e 70000).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf \u003d df.withColumn(\"kill\", col(\"kill\").cast(\"int\"))\ndf \u003d df.withColumn(\"death\", col(\"death\").cast(\"int\"))\ndf \u003d df.withColumn(\"assist\", col(\"assist\").cast(\"int\"))\ndf.createOrReplaceTempView(\"df\")\n\nsql_query \u003d \"\"\"\n    SELECT MAX(assist) as max_kill\n    FROM df\n\"\"\"\nspark.sql(sql_query).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsql_query \u003d \"\"\"\n    SELECT win\n    FROM df\n    WHERE kill \u003d (SELECT MAX(kill) FROM df)\n\"\"\"\n\nspark.sql(sql_query).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf \u003d df.withColumn(\"total_damage_to_champion\", col(\"total_damage_to_champion\").cast(\"int\"))\n\ndf.createOrReplaceTempView(\"df\")\n\n\nsql_query \u003d \"\"\"\n    SELECT *\n    FROM df\n    WHERE total_damage_to_champion \u003d (\n        SELECT MAX(total_damage_to_champion)\n        FROM df\n        )\n\"\"\"\n\nspark.sql(sql_query).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsql_query \u003d \"\"\"\n    SELECT AVG(kill) as kill, AVG(death) as death, AVG(assist) as assist, (AVG(kill)+AVG(assist))/AVG(death) as kda\n    FROM df\n\"\"\"\n\nspark.sql(sql_query).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsql_query \u003d \"\"\"\n    SELECT \n        item, \n        COUNT(*) AS pick_count,\n        AVG(CASE WHEN win \u003d True THEN 1 ELSE 0 END) AS win_rate\n    FROM (\n        SELECT item0 AS item, win FROM df\n        UNION ALL\n        SELECT item1 AS item, win FROM df\n        UNION ALL\n        SELECT item2 AS item, win FROM df\n        UNION ALL\n        SELECT item3 AS item, win FROM df\n        UNION ALL\n        SELECT item4 AS item, win FROM df\n        UNION ALL\n        SELECT item5 AS item, win FROM df\n    ) AS items\n    WHERE item !\u003d 0\n    GROUP BY item\n    ORDER BY pick_count DESC\n\"\"\"\nspark.sql(sql_query).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsql_query \u003d \"\"\"\n    SELECT \n        AVG(CASE WHEN win \u003d True THEN 1 ELSE 0 END) AS overall_win_rate\n    FROM df\n\"\"\"\nspark.sql(sql_query).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}